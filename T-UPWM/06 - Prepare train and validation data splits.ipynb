{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import sys\n",
    "import subprocess\n",
    "import shutil\n",
    "from os.path import join\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from nir.utils import create_filter_query_function, change_bm25_parameters\n",
    "from utils import *\n",
    "import pickle\n",
    "from mmnrm.dataset import TrainCollectionV2, TrainSnippetsCollectionV2, TestCollectionV2\n",
    "from mmnrm.evaluation import BioASQ_Evaluator\n",
    "\n",
    "import json\n",
    "\n",
    "es = Elasticsearch([\"http://193.136.175.98:8125\"])\n",
    "\n",
    "index_name = \"bioasq_9b\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_index, training_data_path = \"training9b_wDates.json\"):\n",
    "    \n",
    "    queries = load_queries(training_data_path, maps=[(\"body\",\"query\")])\n",
    "\n",
    "    test_8b = []\n",
    "\n",
    "    for i in range(1,6):\n",
    "        id_sets = set()\n",
    "        with open(f\"yearly_data/8B{i}_golden.json\",\"r\") as f:\n",
    "            for q in json.load(f)[\"questions\"]:\n",
    "                id_sets.add(q[\"id\"])\n",
    "\n",
    "        test_8b.append(id_sets)\n",
    "\n",
    "    print(sum([len(x) for x in test_8b]))\n",
    "\n",
    "    queries_ids_sets = { x[\"id\"] for x in queries }\n",
    "    train_ids = queries_ids_sets - test_8b[batch_index]\n",
    "    validations_ids = test_8b[batch_index]\n",
    "\n",
    "    train_data = subset_byId(queries, train_ids)\n",
    "    validation_data = subset_byId(queries, validations_ids)\n",
    "\n",
    "    convert_to_trainable_gs = lambda x: { k:{1:v}for k,v in x.items()}\n",
    "\n",
    "    train_data_queries, train_data_gs = separate_queries_goldstandard(train_data, additional_keys=[\"limit_date\"])\n",
    "    train_data_gs = convert_to_trainable_gs(train_data_gs)\n",
    "    print(len(train_data))\n",
    "    validation_data_queries, _ = separate_queries_goldstandard(validation_data, additional_keys=[\"limit_date\"])\n",
    "    \n",
    "    validation_data_gs = list(map(lambda x:{\"id\":x[\"id\"], \n",
    "                                            \"query\":x[\"query\"], \n",
    "                                            \"documents\":[y.split(\"/\")[-1] for y in x[\"documents\"]],\n",
    "                                            \"limit_date\":x[\"limit_date\"]},\n",
    "                                           validation_data))\n",
    "    \n",
    "    print(len(validation_data))\n",
    "    \n",
    "    return train_data_queries, train_data_gs, validation_data_queries, validation_data_gs\n",
    "\n",
    "def load_training_data_WSnippets(batch_index, training_data_path = \"training9b_wDates.json\"):\n",
    "    \n",
    "    def get_snippets_by_docid(snippets, doc_id):\n",
    "        return list(filter(lambda x:x[\"document\"].split(\"/\")[-1]==doc_id and (x[\"offsetInEndSection\"]-x[\"offsetInBeginSection\"])>0 , snippets))\n",
    "    \n",
    "    def separate_queries_goldstandard(queires, additional_keys=[]):\n",
    "        clean_queires = []\n",
    "        gs = {}\n",
    "        additional_keys = [\"id\", \"query\"] + additional_keys\n",
    "        \n",
    "        total_empty = 0\n",
    "        total_empty_query = 0\n",
    "        \n",
    "        for x in queires:\n",
    "            gs[x[\"id\"]] = {}\n",
    "            for doc_id in list(map(lambda y : y.split(\"/\")[-1], x[\"documents\"])):\n",
    "                \n",
    "                snippets_of_docid = get_snippets_by_docid(x[\"snippets\"], doc_id)\n",
    "                \n",
    "                if len(snippets_of_docid)>0:\n",
    "                    gs[x[\"id\"]][doc_id] = snippets_of_docid\n",
    "                else:\n",
    "                    total_empty += 1\n",
    "                \n",
    "            \n",
    "            if len(gs[x[\"id\"]])>0:\n",
    "                clean_queires.append({k:x[k] for k in additional_keys})\n",
    "            else:\n",
    "                # remove this query\n",
    "                total_empty_query += 1\n",
    "                del gs[x[\"id\"]]\n",
    "        \n",
    "        print(\"Num empty queries\",total_empty_query)\n",
    "        print(\"Num docs without snippets\",total_empty)\n",
    "        \n",
    "        \n",
    "        return clean_queires, gs\n",
    "    \n",
    "    \n",
    "    queries = load_queries(training_data_path, maps=[(\"body\",\"query\")])\n",
    "\n",
    "    test_8b = []\n",
    "\n",
    "    for i in range(1,6):\n",
    "        id_sets = set()\n",
    "        with open(f\"yearly_data/8B{i}_golden.json\",\"r\") as f:\n",
    "            for q in json.load(f)[\"questions\"]:\n",
    "                id_sets.add(q[\"id\"])\n",
    "\n",
    "        test_8b.append(id_sets)\n",
    "\n",
    "    print(sum([len(x) for x in test_8b]))\n",
    "\n",
    "    queries_ids_sets = { x[\"id\"] for x in queries }\n",
    "    train_ids = queries_ids_sets - test_8b[batch_index]\n",
    "\n",
    "    train_data = subset_byId(queries, train_ids)\n",
    "\n",
    "    convert_to_trainable_gs = lambda x: { k:{1:v}for k,v in x.items()}\n",
    "\n",
    "    train_data_queries, train_data_gs = separate_queries_goldstandard(train_data, additional_keys=[\"limit_date\"])    \n",
    "    \n",
    "    return train_data_queries, train_data_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_collections(train_data_queries, train_data_gs, validation_data_queries, validation_data_gs, K1, BETA, TOP_N):\n",
    "    \n",
    "    print(K1,BETA,TOP_N)\n",
    "    \n",
    "    query_results = execute_search(es, train_data_queries, TOP_N, index_name, k1=K1, b=BETA)\n",
    "\n",
    "    t_collection = TrainCollectionV2(train_data_queries, \n",
    "                                   train_data_gs, \n",
    "                                   query_results, \n",
    "                                   use_relevance_groups=False)\\\n",
    "                            .batch_size(32)\n",
    "\n",
    "    \n",
    "\n",
    "    ## VALIDATION DATA\n",
    "\n",
    "    query_results = execute_search(es, validation_data_queries, TOP_N, index_name, k1=K1, b=BETA)\n",
    "\n",
    "\n",
    "    evaluator = BioASQ_Evaluator(validation_data_gs)\n",
    "\n",
    "    validation_collection = TestCollectionV2(validation_data_queries, \n",
    "                                               query_results,\n",
    "                                               evaluator)\\\n",
    "                                        .batch_size(32)\n",
    "\n",
    "    return t_collection, validation_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "3643\n",
      "100\n",
      "Setting the k1 and b for BM25\n",
      "Minimum number of relevance type(0) in the queries of the goldstandard sub set: 169\n",
      "Mean number of relevance type(0) in the queries of the goldstandard sub set: 242.0714540907467\n",
      "Minimum number of relevance type(1) in the queries of the goldstandard sub set: 1\n",
      "Mean number of relevance type(1) in the queries of the goldstandard sub set: 7.928545909253304\n",
      "Sub Collection size 544079\n",
      "Number of skipped question, due to lack of true positives 844\n",
      "Setting the k1 and b for BM25\n",
      "Running query: 80\r"
     ]
    }
   ],
   "source": [
    "# make a BM25 search\n",
    "batch_index = 0\n",
    "\n",
    "\n",
    "experiments = [(0.9, 0.4, 100), (0.6, 0.4, 250)]\n",
    "\n",
    "for k1,beta,top_n in experiments:\n",
    "    \n",
    "    train_queries, train_gs, validation_queries, validation_gs = load_data(batch_index)\n",
    "    \n",
    "    t_collection, v_collection = build_data_collections(train_queries, \n",
    "                                                        train_gs, \n",
    "                                                        validation_queries, \n",
    "                                                        validation_gs, \n",
    "                                                        k1, \n",
    "                                                        beta, \n",
    "                                                        top_n)\n",
    "    \n",
    "    t_collection.save(f\"_del_training_batch_0{batch_index+1}_{top_n}\")\n",
    "    v_collection.save(f\"_del_validation_batch_0{batch_index+1}_{top_n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "3643\n",
      "100\n",
      "0.4 0.4 250\n",
      "Setting the k1 and b for BM25\n",
      "Minimum number of relevance type(0) in the queries of the goldstandard sub set: 161\n",
      "Mean number of relevance type(0) in the queries of the goldstandard sub set: 242.3090856814261\n",
      "Minimum number of relevance type(1) in the queries of the goldstandard sub set: 1\n",
      "Mean number of relevance type(1) in the queries of the goldstandard sub set: 7.690914318573893\n",
      "Sub Collection size 665612\n",
      "Number of skipped question, due to lack of true positives 165\n",
      "Setting the k1 and b for BM25\n",
      "500ning query: 80\n",
      "3643\n",
      "100\n",
      "0.9 0.53 100\n",
      "Setting the k1 and b for BM25\n",
      "Minimum number of relevance type(0) in the queries of the goldstandard sub set: 43\n",
      "Mean number of relevance type(0) in the queries of the goldstandard sub set: 93.29015240328253\n",
      "Minimum number of relevance type(1) in the queries of the goldstandard sub set: 1\n",
      "Mean number of relevance type(1) in the queries of the goldstandard sub set: 6.709847596717467\n",
      "Sub Collection size 280215\n",
      "Number of skipped question, due to lack of true positives 231\n",
      "Setting the k1 and b for BM25\n",
      "500ning query: 80\n",
      "3643\n",
      "100\n",
      "0.3 0.67 100\n",
      "Setting the k1 and b for BM25\n",
      "Minimum number of relevance type(0) in the queries of the goldstandard sub set: 42\n",
      "Mean number of relevance type(0) in the queries of the goldstandard sub set: 93.17913094886195\n",
      "Minimum number of relevance type(1) in the queries of the goldstandard sub set: 1\n",
      "Mean number of relevance type(1) in the queries of the goldstandard sub set: 6.820869051138043\n",
      "Sub Collection size 281031\n",
      "Number of skipped question, due to lack of true positives 260\n",
      "Setting the k1 and b for BM25\n",
      "Running query: 80\r"
     ]
    }
   ],
   "source": [
    "# make a BM25 search\n",
    "batch_index = 1\n",
    "\n",
    "\n",
    "experiments = [(0.4, 0.4, 250), (0.9, 0.53, 100), (0.3, 0.67, 100)]\n",
    "\n",
    "for k1,beta,top_n in experiments:\n",
    "    \n",
    "    train_queries, train_gs, validation_queries, validation_gs = load_data(batch_index)\n",
    "    \n",
    "    t_collection, v_collection = build_data_collections(train_queries, \n",
    "                                                        train_gs, \n",
    "                                                        validation_queries, \n",
    "                                                        validation_gs, \n",
    "                                                        k1, \n",
    "                                                        beta, \n",
    "                                                        top_n)\n",
    "    \n",
    "    t_collection.save(f\"training_batch_0{batch_index+1}_{k1}_{beta}_{top_n}\")\n",
    "    v_collection.save(f\"validation_batch_0{batch_index+1}_{k1}_{beta}_{top_n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "3643\n",
      "100\n",
      "0.4 0.14 250\n",
      "Setting the k1 and b for BM25\n",
      "The inquery limit_date will be used\n",
      "Minimum number of relevance type(0) in the queries of the goldstandard sub set: 160\n",
      "Mean number of relevance type(0) in the queries of the goldstandard sub set: 242.31793400286944\n",
      "Minimum number of relevance type(1) in the queries of the goldstandard sub set: 1\n",
      "Mean number of relevance type(1) in the queries of the goldstandard sub set: 7.682065997130559\n",
      "Sub Collection size 660164\n",
      "Number of skipped question, due to lack of true positives 158\n",
      "Setting the k1 and b for BM25\n",
      "The inquery limit_date will be used\n",
      "500ning query: 80\n",
      "3643\n",
      "100\n",
      "0.9 0.09 100\n",
      "Setting the k1 and b for BM25\n",
      "The inquery limit_date will be used\n",
      "Minimum number of relevance type(0) in the queries of the goldstandard sub set: 44\n",
      "Mean number of relevance type(0) in the queries of the goldstandard sub set: 93.27347659699736\n",
      "Minimum number of relevance type(1) in the queries of the goldstandard sub set: 1\n",
      "Mean number of relevance type(1) in the queries of the goldstandard sub set: 6.726523403002649\n",
      "Sub Collection size 276014\n",
      "Number of skipped question, due to lack of true positives 246\n",
      "Setting the k1 and b for BM25\n",
      "The inquery limit_date will be used\n",
      "500ning query: 80\n",
      "3643\n",
      "100\n",
      "0.5 0.79 100\n",
      "Setting the k1 and b for BM25\n",
      "The inquery limit_date will be used\n",
      "Minimum number of relevance type(0) in the queries of the goldstandard sub set: 42\n",
      "Mean number of relevance type(0) in the queries of the goldstandard sub set: 93.2180052956752\n",
      "Minimum number of relevance type(1) in the queries of the goldstandard sub set: 1\n",
      "Mean number of relevance type(1) in the queries of the goldstandard sub set: 6.781994704324801\n",
      "Sub Collection size 281645\n",
      "Number of skipped question, due to lack of true positives 244\n",
      "Setting the k1 and b for BM25\n",
      "The inquery limit_date will be used\n",
      "Running query: 80\r"
     ]
    }
   ],
   "source": [
    "# make a BM25 search\n",
    "batch_index = 2\n",
    "\n",
    "\n",
    "experiments = [(0.4, 0.14, 250), (0.9, 0.09, 100), (0.5, 0.79, 100)]\n",
    "\n",
    "for k1,beta,top_n in experiments:\n",
    "    \n",
    "    train_queries, train_gs, validation_queries, validation_gs = load_data(batch_index)\n",
    "    \n",
    "    t_collection, v_collection = build_data_collections(train_queries, \n",
    "                                                        train_gs, \n",
    "                                                        validation_queries, \n",
    "                                                        validation_gs, \n",
    "                                                        k1, \n",
    "                                                        beta, \n",
    "                                                        top_n)\n",
    "    \n",
    "    t_collection.save(f\"training_batch_0{batch_index+1}_{k1}_{beta}_{top_n}\")\n",
    "    v_collection.save(f\"validation_batch_0{batch_index+1}_{k1}_{beta}_{top_n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "3643\n",
      "100\n",
      "0.6 0.18 250\n",
      "Setting the k1 and b for BM25\n",
      "The inquery limit_date will be used\n",
      "Minimum number of relevance type(0) in the queries of the goldstandard sub set: 161\n",
      "Mean number of relevance type(0) in the queries of the goldstandard sub set: 242.33486107132626\n",
      "Minimum number of relevance type(1) in the queries of the goldstandard sub set: 1\n",
      "Mean number of relevance type(1) in the queries of the goldstandard sub set: 7.665138928673732\n",
      "Sub Collection size 660263\n",
      "Number of skipped question, due to lack of true positives 152\n",
      "Setting the k1 and b for BM25\n",
      "The inquery limit_date will be used\n",
      "500ning query: 80\n",
      "3643\n",
      "100\n",
      "0.9 0.26 100\n",
      "Setting the k1 and b for BM25\n",
      "The inquery limit_date will be used\n",
      "Minimum number of relevance type(0) in the queries of the goldstandard sub set: 43\n",
      "Mean number of relevance type(0) in the queries of the goldstandard sub set: 93.24625330590655\n",
      "Minimum number of relevance type(1) in the queries of the goldstandard sub set: 1\n",
      "Mean number of relevance type(1) in the queries of the goldstandard sub set: 6.753746694093447\n",
      "Sub Collection size 277577\n",
      "Number of skipped question, due to lack of true positives 240\n",
      "Setting the k1 and b for BM25\n",
      "The inquery limit_date will be used\n",
      "500ning query: 80\n",
      "3643\n",
      "100\n",
      "0.6 0.69 100\n",
      "Setting the k1 and b for BM25\n",
      "The inquery limit_date will be used\n",
      "Minimum number of relevance type(0) in the queries of the goldstandard sub set: 43\n",
      "Mean number of relevance type(0) in the queries of the goldstandard sub set: 93.20258367586612\n",
      "Minimum number of relevance type(1) in the queries of the goldstandard sub set: 1\n",
      "Mean number of relevance type(1) in the queries of the goldstandard sub set: 6.797416324133882\n",
      "Sub Collection size 280958\n",
      "Number of skipped question, due to lack of true positives 237\n",
      "Setting the k1 and b for BM25\n",
      "The inquery limit_date will be used\n",
      "Running query: 80\r"
     ]
    }
   ],
   "source": [
    "# make a BM25 search\n",
    "batch_index = 3\n",
    "\n",
    "\n",
    "experiments = [(0.6, 0.18, 250), (0.9, 0.26, 100), (0.6, 0.69, 100)]\n",
    "\n",
    "for k1,beta,top_n in experiments:\n",
    "    \n",
    "    train_queries, train_gs, validation_queries, validation_gs = load_data(batch_index)\n",
    "    \n",
    "    t_collection, v_collection = build_data_collections(train_queries, \n",
    "                                                        train_gs, \n",
    "                                                        validation_queries, \n",
    "                                                        validation_gs, \n",
    "                                                        k1, \n",
    "                                                        beta, \n",
    "                                                        top_n)\n",
    "    \n",
    "    t_collection.save(f\"training_batch_0{batch_index+1}_{k1}_{beta}_{top_n}\")\n",
    "    v_collection.save(f\"validation_batch_0{batch_index+1}_{k1}_{beta}_{top_n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "3643\n",
      "100\n",
      "0.6 0.51 250\n",
      "Setting the k1 and b for BM25\n",
      "The inquery limit_date will be used\n",
      "Minimum number of relevance type(0) in the queries of the goldstandard sub set: 164\n",
      "Mean number of relevance type(0) in the queries of the goldstandard sub set: 242.29238258877433\n",
      "Minimum number of relevance type(1) in the queries of the goldstandard sub set: 1\n",
      "Mean number of relevance type(1) in the queries of the goldstandard sub set: 7.707617411225659\n",
      "Sub Collection size 668526\n",
      "Number of skipped question, due to lack of true positives 151\n",
      "Setting the k1 and b for BM25\n",
      "The inquery limit_date will be used\n",
      "500ning query: 80\n",
      "3643\n",
      "100\n",
      "0.5 0.15 100\n",
      "Setting the k1 and b for BM25\n",
      "The inquery limit_date will be used\n",
      "Minimum number of relevance type(0) in the queries of the goldstandard sub set: 44\n",
      "Mean number of relevance type(0) in the queries of the goldstandard sub set: 93.12040035325288\n",
      "Minimum number of relevance type(1) in the queries of the goldstandard sub set: 1\n",
      "Mean number of relevance type(1) in the queries of the goldstandard sub set: 6.879599646747129\n",
      "Sub Collection size 278734\n",
      "Number of skipped question, due to lack of true positives 246\n",
      "Setting the k1 and b for BM25\n",
      "The inquery limit_date will be used\n",
      "500ning query: 80\n",
      "3643\n",
      "100\n",
      "0.4 0.44 100\n",
      "Setting the k1 and b for BM25\n",
      "The inquery limit_date will be used\n",
      "Minimum number of relevance type(0) in the queries of the goldstandard sub set: 42\n",
      "Mean number of relevance type(0) in the queries of the goldstandard sub set: 93.10011778563015\n",
      "Minimum number of relevance type(1) in the queries of the goldstandard sub set: 1\n",
      "Mean number of relevance type(1) in the queries of the goldstandard sub set: 6.899882214369847\n",
      "Sub Collection size 280953\n",
      "Number of skipped question, due to lack of true positives 247\n",
      "Setting the k1 and b for BM25\n",
      "The inquery limit_date will be used\n",
      "Running query: 80\r"
     ]
    }
   ],
   "source": [
    "# make a BM25 search\n",
    "batch_index = 4\n",
    "\n",
    "\n",
    "experiments = [(0.6, 0.51, 250), (0.5, 0.15, 100), (0.4, 0.44, 100)]\n",
    "\n",
    "for k1,beta,top_n in experiments:\n",
    "    \n",
    "    train_queries, train_gs, validation_queries, validation_gs = load_data(batch_index)\n",
    "    \n",
    "    t_collection, v_collection = build_data_collections(train_queries, \n",
    "                                                        train_gs, \n",
    "                                                        validation_queries, \n",
    "                                                        validation_gs, \n",
    "                                                        k1, \n",
    "                                                        beta, \n",
    "                                                        top_n)\n",
    "    \n",
    "    t_collection.save(f\"training_batch_0{batch_index+1}_{k1}_{beta}_{top_n}\")\n",
    "    v_collection.save(f\"validation_batch_0{batch_index+1}_{k1}_{beta}_{top_n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "Num empty queries 0\n",
      "Num docs without snippets 6869\n"
     ]
    }
   ],
   "source": [
    "# snippets training data\n",
    "batch_index = 4 # round 4\n",
    "\n",
    "train_queries, train_gs = load_training_data_WSnippets(batch_index)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the k1 and b for BM25\n",
      "The inquery limit_date will be used\n",
      "buildng query: 3640\n",
      "index to remove length 183\n",
      "Minimum number of relevance type(0) in the queries of the goldstandard sub set: 192\n",
      "Mean number of relevance type(0) in the queries of the goldstandard sub set: 243.62687861271675\n",
      "Minimum number of relevance type(1) in the queries of the goldstandard sub set: 1\n",
      "Mean number of relevance type(1) in the queries of the goldstandard sub set: 6.373121387283237\n",
      "Sub Collection size 663270\n",
      "Number of skipped question, due to lack of true positives 183\n"
     ]
    }
   ],
   "source": [
    "K1,BETA,TOP_N = (0.6, 0.51, 250)\n",
    "\n",
    "query_results = execute_search(es, train_queries, TOP_N, index_name, k1=K1, b=BETA)\n",
    "print(\"build\")\n",
    "t_collection = TrainSnippetsCollectionV2(train_queries, \n",
    "                               train_gs, \n",
    "                               query_results,\n",
    "                               use_soft_label=False,\n",
    "                               use_relevance_groups=False)\\\n",
    "                        .batch_size(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_collection.save(f\"joint_training_hardlabel_batch_0{batch_index+1}_{K1}_{BETA}_{TOP_N}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the k1 and b for BM25\n",
      "Running query: 80\r"
     ]
    }
   ],
   "source": [
    "TOP_N = 25\n",
    "\n",
    "query_results = execute_search(es, validation_data_queries, TOP_N, index_name, k1=0.6, b=0.4)\n",
    "\n",
    "validation_data_gs = list(map(lambda x:{\"id\":x[\"id\"], \n",
    "                                        \"query\":x[\"query\"], \n",
    "                                        \"documents\":[y.split(\"/\")[-1] for y in x[\"documents\"]],\n",
    "                                        \"limit_date\":x[\"limit_date\"]},\n",
    "                                       validation_data))\n",
    "\n",
    "evaluator = BioASQ_Evaluator(validation_data_gs)\n",
    "\n",
    "validation_collection = TestCollectionV2(validation_data_queries, \n",
    "                                           query_results,\n",
    "                                           evaluator)\\\n",
    "                                    .batch_size(32)\n",
    "\n",
    "validation_collection.save(\"validation_data_batch_01_\"+str(TOP_N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N = 25\n",
    "\n",
    "query_results = execute_search(es, validation_data_queries, TOP_N, index_name, k1=0.6, b=0.4)\n",
    "\n",
    "validation_data_gs = list(map(lambda x:{\"id\":x[\"id\"], \n",
    "                                        \"query\":x[\"query\"], \n",
    "                                        \"documents\":[y.split(\"/\")[-1] for y in x[\"documents\"]],\n",
    "                                        \"limit_date\":x[\"limit_date\"]},\n",
    "                                       validation_data))\n",
    "\n",
    "evaluator = BioASQ_Evaluator(validation_data_gs)\n",
    "\n",
    "validation_collection = TestCollectionV2(validation_data_queries, \n",
    "                                           query_results,\n",
    "                                           evaluator)\\\n",
    "                                    .batch_size(32)\n",
    "\n",
    "validation_collection.save(\"validation_data_batch_01_\"+str(TOP_N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the k1 and b for BM25\n",
      "Minimum number of relevance type(0) in the queries of the goldstandard sub set: 32\n",
      "Mean number of relevance type(0) in the queries of the goldstandard sub set: 43.758620689655174\n",
      "Minimum number of relevance type(1) in the queries of the goldstandard sub set: 1\n",
      "Mean number of relevance type(1) in the queries of the goldstandard sub set: 6.241379310344827\n",
      "Sub Collection size 1450\n",
      "Number of skipped question, due to lack of true positives 3\n"
     ]
    }
   ],
   "source": [
    "TOP_N = 50\n",
    "\n",
    "train_data_queries = train_data_queries[:32]\n",
    "\n",
    "query_results = execute_search(es, train_data_queries, TOP_N, index_name, k1=0.6, b=0.4)\n",
    "\n",
    "t_collection = TrainCollectionV2(train_data_queries, \n",
    "                               train_data_gs, \n",
    "                               query_results, \n",
    "                               use_relevance_groups=False)\\\n",
    "                        .batch_size(32)\n",
    "\n",
    "t_collection.save(\"training_batch_01_\"+str(TOP_N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-bioasq",
   "language": "python",
   "name": "py-bioasq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
