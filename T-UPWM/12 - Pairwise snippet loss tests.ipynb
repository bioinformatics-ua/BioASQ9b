{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from mmnrm.utils import set_random_seed, load_neural_model, load_model, load_sentence_generator, flat_list\n",
    "from nir.embeddings import FastText, Word2Vec\n",
    "\n",
    "set_random_seed()\n",
    "\n",
    "import io\n",
    "from nir.tokenizers import Regex, BioCleanTokenizer, BioCleanTokenizer2, Regex2\n",
    "import numpy as np\n",
    "import math\n",
    "import os \n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from mmnrm.dataset import TrainCollectionV2, TrainSnippetsCollectionV2, TestCollectionV2, sentence_splitter_builderV2, TrainPairwiseCollection\n",
    "from mmnrm.modelsv2 import sibm2_wSnippets\n",
    "from mmnrm.callbacks import TriangularLR, WandBValidationLogger, LearningRateScheduler\n",
    "from mmnrm.training import PairwiseTraining, pairwise_cross_entropy\n",
    "from mmnrm.utils import merge_dicts\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "def build_data_generators(tokenizer, queries_sw=None, docs_sw=None):\n",
    "    \n",
    "    punkt_sent_tokenizer = PunktSentenceTokenizer().span_tokenize\n",
    "    \n",
    "    def sent_tokenize(document):\n",
    "        return [ document[start:end] for start,end in punkt_sent_tokenizer(document) ]\n",
    "    \n",
    "    def maybe_tokenize(documents):\n",
    "        if \"tokens\" not in documents:\n",
    "            split = sent_tokenize(documents[\"text\"])\n",
    "            documents[\"tokens\"] = tokenizer.texts_to_sequences(split)\n",
    "            if docs_sw is not None:\n",
    "                for tokenized_sentence in documents[\"tokens\"]:\n",
    "                    tokenized_sentence = [token for token in tokenized_sentence if token not in docs_sw]\n",
    "    \n",
    "    def maybe_tokenize_train(documents):\n",
    "        if \"tokens\" not in documents:\n",
    "            #split = nltk.sent_tokenize(documents[\"text\"])\n",
    "            documents[\"tokens\"] = tokenizer.texts_to_sequences(map(lambda x: x[\"text\"],documents[\"snippets\"]))\n",
    "            if docs_sw is not None:\n",
    "                for tokenized_sentence in documents[\"tokens\"]:\n",
    "                    tokenized_sentence = [token for token in tokenized_sentence if token not in docs_sw]\n",
    "    \n",
    "    def train_generator(data_generator):\n",
    "        while True:\n",
    "\n",
    "            # get the batch triplet\n",
    "            query, pos_docs, pos_label, neg_docs = next(data_generator)\n",
    "\n",
    "            # tokenization, this can be cached for efficientcy porpuses NOTE!!\n",
    "            tokenized_query = tokenizer.texts_to_sequences(query)\n",
    "\n",
    "            if queries_sw is not None:\n",
    "                for tokens in tokenized_query:\n",
    "                    tokenized_query = [token for token in tokens if token not in queries_sw] \n",
    "            \n",
    "            saveReturn = True\n",
    "            \n",
    "            for batch_index in range(len(pos_docs)):\n",
    "                \n",
    "                # tokenizer with cache in [batch_index][tokens]\n",
    "                maybe_tokenize_train(pos_docs[batch_index])\n",
    "                \n",
    "                # assertion\n",
    "                if all([ len(sentence)==0  for sentence in pos_docs[batch_index][\"tokens\"]]):\n",
    "                    saveReturn = False\n",
    "                    break # try a new resampling, NOTE THIS IS A EASY FIX PLS REDO THIS!!!!!!!\n",
    "                          # for obvious reasons\n",
    "                \n",
    "                maybe_tokenize_train(neg_docs[batch_index])\n",
    "                \n",
    "            if saveReturn: # this is not true, if the batch is rejected\n",
    "                yield tokenized_query, pos_docs, pos_label, neg_docs\n",
    "\n",
    "    def test_generator(data_generator):\n",
    "        for _id, query, docs in data_generator:\n",
    "            tokenized_queries = []\n",
    "            for i in range(len(_id)):\n",
    "                # tokenization\n",
    "                tokenized_query = tokenizer.texts_to_sequences([query[i]])[0]\n",
    "\n",
    "                if queries_sw is not None:\n",
    "                    tokenized_query = [token for token in tokenized_query if token not in queries_sw] \n",
    "                \n",
    "                tokenized_queries.append(tokenized_query)\n",
    "                    \n",
    "        \n",
    "                for doc in docs[i]:\n",
    "                    maybe_tokenize(doc)\n",
    "                                                 \n",
    "            yield _id, tokenized_queries, docs\n",
    "            \n",
    "    return train_generator, test_generator\n",
    "\n",
    "def model_train_generator_for_model(model):\n",
    "\n",
    "    if \"model\" in model.savable_config:\n",
    "        cfg = model.savable_config[\"model\"]\n",
    "    \n",
    "    train_gen, test_gen = build_data_generators(model.tokenizer)\n",
    "    \n",
    "    pad_tokens = lambda x, max_len, dtype='int32': tf.keras.preprocessing.sequence.pad_sequences(x, \n",
    "                                                                                           maxlen=max_len,\n",
    "                                                                                           dtype=dtype, \n",
    "                                                                                           padding='post', \n",
    "                                                                                           truncating='post', \n",
    "                                                                                           value=0)\n",
    "\n",
    "    pad_sentences = lambda x, max_lim, dtype='int32': x[:max_lim] + [[]]*(max_lim-len(x))\n",
    "    \n",
    "    pad_labels = lambda x, max_lim, dtype='int32': x[:max_lim] + [0]*(max_lim-len(x))\n",
    "    \n",
    "    def maybe_padding(document, labels = None):\n",
    "        if isinstance(document[\"tokens\"], list):\n",
    "            #overflow prevention\n",
    "            bounded_doc_passage = min(cfg[\"max_passages\"],len(document[\"tokens\"]))\n",
    "            document[\"sentences_mask\"] = [True] * bounded_doc_passage + [False] * (cfg[\"max_passages\"]-bounded_doc_passage)\n",
    "            document[\"tokens\"] = pad_tokens(pad_sentences(document[\"tokens\"], cfg[\"max_passages\"]), cfg[\"max_p_terms\"])\n",
    "            if labels is not None:\n",
    "                document[\"sentences_labels\"] = pad_labels(labels, cfg[\"max_passages\"])\n",
    "                \n",
    "    def train_generator(data_generator):\n",
    " \n",
    "        for query, pos_docs, pos_label, neg_docs in train_gen(data_generator):\n",
    "            \n",
    "            query = pad_tokens(query, cfg[\"max_q_terms\"])\n",
    "            \n",
    "            pos_docs_array = []\n",
    "            pos_snippets_labels = []\n",
    "            pos_docs_mask_array = []\n",
    "            neg_docs_array = []\n",
    "            neg_docs_mask_array = []\n",
    "            \n",
    "            # pad docs, use cache here\n",
    "            for batch_index in range(len(pos_docs)):\n",
    "                maybe_padding(pos_docs[batch_index], pos_label[batch_index])\n",
    "                pos_docs_array.append(pos_docs[batch_index][\"tokens\"])\n",
    "                pos_snippets_labels.append(pos_docs[batch_index][\"sentences_labels\"])\n",
    "                pos_docs_mask_array.append(pos_docs[batch_index][\"sentences_mask\"])\n",
    "                maybe_padding(neg_docs[batch_index])\n",
    "                neg_docs_array.append(neg_docs[batch_index][\"tokens\"])\n",
    "                neg_docs_mask_array.append(neg_docs[batch_index][\"sentences_mask\"])\n",
    "            \n",
    "            yield [query, np.array(pos_docs_array), np.array(pos_docs_mask_array), np.array(pos_snippets_labels)], [query, np.array(neg_docs_array), np.array(neg_docs_mask_array)]\n",
    "            \n",
    "    def test_generator(data_generator):\n",
    "        \n",
    "        for ids, query, docs in test_gen(data_generator):\n",
    "            \n",
    "            docs_ids = []\n",
    "            docs_array = []\n",
    "            docs_mask_array = []\n",
    "            query_array = []\n",
    "            query_ids = []\n",
    "            \n",
    "            for i in range(len(ids)):\n",
    "                \n",
    "                for doc in docs[i]:\n",
    "                    # pad docs, use cache here\n",
    "                    maybe_padding(doc)\n",
    "                    docs_array.append(doc[\"tokens\"])\n",
    "                    docs_mask_array.append(doc[\"sentences_mask\"])\n",
    "                    docs_ids.append(doc[\"id\"])\n",
    "                \n",
    "                query_tokens = pad_tokens([query[i]], cfg[\"max_q_terms\"])[0]\n",
    "                query_tokens = [query_tokens] * len(docs[i])\n",
    "                query_array.append(query_tokens)\n",
    "                    \n",
    "                query_ids.append([ids[i]]*len(docs[i]))\n",
    "            \n",
    "            #print(np.array(docs_mask_array))\n",
    "            \n",
    "            yield flat_list(query_ids), [np.array(flat_list(query_array)), np.array(docs_array), np.array(docs_mask_array)], docs_ids, None\n",
    "            \n",
    "    return train_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "min_freq = 0\n",
    "mun_itter = 15\n",
    "emb_size = 200\n",
    "\n",
    "train_batch_size=32\n",
    "type_split_mode=4\n",
    "use_query_sw = False\n",
    "use_docs_sw = False\n",
    "\n",
    "cache_folder = \"/backup/BioASQ-9b\"\n",
    "index_name = \"bioasq_9b\"\n",
    "\n",
    "tokenizer_class = Regex\n",
    "tokenizer_cfg = {\"class\":tokenizer_class,\n",
    "                    \"attr\":{\n",
    "                        \"cache_folder\": os.path.join(cache_folder, \"tokenizers\"),\n",
    "                        \"prefix_name\": index_name\n",
    "                    },\n",
    "                    \"min_freq\":min_freq}\n",
    "\n",
    "embeddind_class = Word2Vec\n",
    "embedding_cfg = {\n",
    "    \"class\":embeddind_class,\n",
    "    \"attr\":{\n",
    "        \"cache_folder\": os.path.join(cache_folder, \"embeddings\"),\n",
    "        \"prefix_name\":index_name,\n",
    "        \"path\":\"/backup/pre-trained_embeddings/word2vec/\"+index_name+\"_gensim_iter_\"+str(mun_itter)+\"_freq\"+str(min_freq)+\"_\"+str(emb_size)+\"_\"+tokenizer_class.__name__+\"_word2vec.bin\",\n",
    "    }\n",
    "}\n",
    "\n",
    "model_cfg = {\n",
    "    \"max_q_terms\": 50,\n",
    "    \"max_passages\": 20,\n",
    "    \"max_p_terms\": 70,\n",
    "    \"filters\": 16,\n",
    "    \"match_threshold\": 0.99,\n",
    "    \"activation\": \"mish\",\n",
    "    \"use_mlp_sentence_scores\": False,\n",
    "    \"use_cnn_sentence_scores\": True,\n",
    "    \"top_k_list\": [3, 5, 10, 15],\n",
    "    \"use_avg_pool\":True,\n",
    "    \"use_kmax_avg_pool\":True,\n",
    "    \"semantic_normalized_query_match\" : False,\n",
    "}\n",
    "\n",
    "cfg = {\"model\":model_cfg, \"tokenizer\": tokenizer_cfg, \"embedding\": embedding_cfg}\n",
    "\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "ranking_model = sibm2_wSnippets(**cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_generator, test_input_generator = model_train_generator_for_model(ranking_model)\n",
    "\n",
    "training_data_used = \"joint_training_hardlabel_batch_05_0.6_0.51_250\"\n",
    "train_collection = TrainSnippetsCollectionV2\\\n",
    "                            .load(training_data_used)\\\n",
    "                            .batch_size(train_batch_size)\\\n",
    "                            .set_transform_inputs_fn(train_input_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_doc, neg_doc = next(train_collection.generator()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pos_doc = pos_doc[:3]\n",
    "label_pos_doc_snppets = pos_doc[3]\n",
    "\n",
    "pos = ranking_model(y_pos_doc)\n",
    "neg = ranking_model(neg_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.6818266>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## pairwise doc loss\n",
    "\n",
    "p_wise_loss = pairwise_cross_entropy(pos_doc_score, neg_doc_score)\n",
    "p_wise_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pairwise snippet loss\n",
    "\n",
    "pos_sentence_labels = tf.cast(tf.reshape(label_pos_doc_snppets, (-1, model_cfg[\"max_passages\"], 1)), tf.int32)\n",
    "pos_sentence_scores = tf.cast(tf.math.exp(tf.reshape(pos_snippet_score, (-1, model_cfg[\"max_passages\"], 1))), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformations_for_pairwise(vector):\n",
    "        \n",
    "    vector_t = tf.transpose(vector, perm=[0,2,1])\n",
    "    vector_repeat = tf.repeat(vector, model_cfg[\"max_passages\"], axis=-1)\n",
    "    vector_t_repeat = tf.repeat(vector_t, model_cfg[\"max_passages\"], axis=-2)\n",
    "\n",
    "    vector_xor = tf.cast(tf.math.logical_xor(tf.cast(vector_repeat, tf.bool), \n",
    "                                                          tf.cast(vector_t_repeat, tf.bool)), \n",
    "                                      tf.float32)\n",
    "\n",
    "    vector_repeat = tf.cast(vector_repeat, tf.float32)\n",
    "    vector_t_repeat = tf.cast(vector_t_repeat, tf.float32)\n",
    "\n",
    "    return vector_repeat, vector_t_repeat, vector_xor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sentence_labels_repeat, pos_sentence_labels_repeat_transpose, pos_sentence_lables_xor =  transformations_for_pairwise(pos_sentence_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sentence_scores_repeat, pos_sentence_scores_repeat_transpose, pos_sentence_scores_xor =  transformations_for_pairwise(pos_sentence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.        1.5615495 1.5615495 1.5615495 0.        0.        0.\n",
      "  0.        1.5615495 1.5615495 0.        1.5615495 1.5615495 1.5615495\n",
      "  1.5615495 1.5615495 1.5615495 1.5615495 1.5615495 1.5615495]\n",
      " [1.5615495 0.        0.        0.        1.5319595 1.5319486 1.5318403\n",
      "  1.5318676 0.        0.        1.5172572 0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.       ]\n",
      " [1.5615495 0.        0.        0.        1.5319595 1.5319486 1.5318403\n",
      "  1.5318676 0.        0.        1.5172572 0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.       ]\n",
      " [1.5615495 0.        0.        0.        1.5319595 1.5319486 1.5318403\n",
      "  1.5318676 0.        0.        1.5172572 0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.       ]\n",
      " [0.        1.5319595 1.5319595 1.5319595 0.        0.        0.\n",
      "  0.        1.5319595 1.5319595 0.        1.5319595 1.5319595 1.5319595\n",
      "  1.5319595 1.5319595 1.5319595 1.5319595 1.5319595 1.5319595]\n",
      " [0.        1.5319486 1.5319486 1.5319486 0.        0.        0.\n",
      "  0.        1.5319486 1.5319486 0.        1.5319486 1.5319486 1.5319486\n",
      "  1.5319486 1.5319486 1.5319486 1.5319486 1.5319486 1.5319486]\n",
      " [0.        1.5318403 1.5318403 1.5318403 0.        0.        0.\n",
      "  0.        1.5318403 1.5318403 0.        1.5318403 1.5318403 1.5318403\n",
      "  1.5318403 1.5318403 1.5318403 1.5318403 1.5318403 1.5318403]\n",
      " [0.        1.5318676 1.5318676 1.5318676 0.        0.        0.\n",
      "  0.        1.5318676 1.5318676 0.        1.5318676 1.5318676 1.5318676\n",
      "  1.5318676 1.5318676 1.5318676 1.5318676 1.5318676 1.5318676]\n",
      " [1.5615495 0.        0.        0.        1.5319595 1.5319486 1.5318403\n",
      "  1.5318676 0.        0.        1.5172572 0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.       ]\n",
      " [1.5615495 0.        0.        0.        1.5319595 1.5319486 1.5318403\n",
      "  1.5318676 0.        0.        1.5172572 0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.       ]\n",
      " [0.        1.5172572 1.5172572 1.5172572 0.        0.        0.\n",
      "  0.        1.5172572 1.5172572 0.        1.5172572 1.5172572 1.5172572\n",
      "  1.5172572 1.5172572 1.5172572 1.5172572 1.5172572 1.5172572]\n",
      " [1.5615495 0.        0.        0.        1.5319595 1.5319486 1.5318403\n",
      "  1.5318676 0.        0.        1.5172572 0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.       ]\n",
      " [1.5615495 0.        0.        0.        1.5319595 1.5319486 1.5318403\n",
      "  1.5318676 0.        0.        1.5172572 0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.       ]\n",
      " [1.5615495 0.        0.        0.        1.5319595 1.5319486 1.5318403\n",
      "  1.5318676 0.        0.        1.5172572 0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.       ]\n",
      " [1.5615495 0.        0.        0.        1.5319595 1.5319486 1.5318403\n",
      "  1.5318676 0.        0.        1.5172572 0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.       ]\n",
      " [1.5615495 0.        0.        0.        1.5319595 1.5319486 1.5318403\n",
      "  1.5318676 0.        0.        1.5172572 0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.       ]\n",
      " [1.5615495 0.        0.        0.        1.5319595 1.5319486 1.5318403\n",
      "  1.5318676 0.        0.        1.5172572 0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.       ]\n",
      " [1.5615495 0.        0.        0.        1.5319595 1.5319486 1.5318403\n",
      "  1.5318676 0.        0.        1.5172572 0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.       ]\n",
      " [1.5615495 0.        0.        0.        1.5319595 1.5319486 1.5318403\n",
      "  1.5318676 0.        0.        1.5172572 0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.       ]\n",
      " [1.5615495 0.        0.        0.        1.5319595 1.5319486 1.5318403\n",
      "  1.5318676 0.        0.        1.5172572 0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.       ]], shape=(20, 20), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.0000000e-07 3.0950313e+00 3.0648165e+00 3.1229978e+00 1.0000000e-07\n",
      "  1.0000000e-07 1.0000000e-07 1.0000000e-07 3.0935585e+00 3.0802903e+00\n",
      "  1.0000000e-07 3.0950506e+00 3.1034946e+00 3.1034946e+00 3.1034946e+00\n",
      "  3.1034946e+00 3.1034946e+00 3.1034946e+00 3.1034946e+00 3.1034946e+00]\n",
      " [3.0950313e+00 1.0000000e-07 1.0000000e-07 1.0000000e-07 3.0654414e+00\n",
      "  3.0654304e+00 3.0653222e+00 3.0653496e+00 1.0000000e-07 1.0000000e-07\n",
      "  3.0507390e+00 1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07\n",
      "  1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07]\n",
      " [3.0648165e+00 1.0000000e-07 1.0000000e-07 1.0000000e-07 3.0352266e+00\n",
      "  3.0352156e+00 3.0351074e+00 3.0351348e+00 1.0000000e-07 1.0000000e-07\n",
      "  3.0205243e+00 1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07\n",
      "  1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07]\n",
      " [3.1229978e+00 1.0000000e-07 1.0000000e-07 1.0000000e-07 3.0934079e+00\n",
      "  3.0933969e+00 3.0932887e+00 3.0933161e+00 1.0000000e-07 1.0000000e-07\n",
      "  3.0787055e+00 1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07\n",
      "  1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07]\n",
      " [1.0000000e-07 3.0654414e+00 3.0352266e+00 3.0934079e+00 1.0000000e-07\n",
      "  1.0000000e-07 1.0000000e-07 1.0000000e-07 3.0639687e+00 3.0507002e+00\n",
      "  1.0000000e-07 3.0654607e+00 3.0739045e+00 3.0739045e+00 3.0739045e+00\n",
      "  3.0739045e+00 3.0739045e+00 3.0739045e+00 3.0739045e+00 3.0739045e+00]\n",
      " [1.0000000e-07 3.0654304e+00 3.0352156e+00 3.0933969e+00 1.0000000e-07\n",
      "  1.0000000e-07 1.0000000e-07 1.0000000e-07 3.0639577e+00 3.0506892e+00\n",
      "  1.0000000e-07 3.0654497e+00 3.0738935e+00 3.0738935e+00 3.0738935e+00\n",
      "  3.0738935e+00 3.0738935e+00 3.0738935e+00 3.0738935e+00 3.0738935e+00]\n",
      " [1.0000000e-07 3.0653222e+00 3.0351074e+00 3.0932887e+00 1.0000000e-07\n",
      "  1.0000000e-07 1.0000000e-07 1.0000000e-07 3.0638494e+00 3.0505810e+00\n",
      "  1.0000000e-07 3.0653415e+00 3.0737853e+00 3.0737853e+00 3.0737853e+00\n",
      "  3.0737853e+00 3.0737853e+00 3.0737853e+00 3.0737853e+00 3.0737853e+00]\n",
      " [1.0000000e-07 3.0653496e+00 3.0351348e+00 3.0933161e+00 1.0000000e-07\n",
      "  1.0000000e-07 1.0000000e-07 1.0000000e-07 3.0638766e+00 3.0506082e+00\n",
      "  1.0000000e-07 3.0653687e+00 3.0738125e+00 3.0738125e+00 3.0738125e+00\n",
      "  3.0738125e+00 3.0738125e+00 3.0738125e+00 3.0738125e+00 3.0738125e+00]\n",
      " [3.0935585e+00 1.0000000e-07 1.0000000e-07 1.0000000e-07 3.0639687e+00\n",
      "  3.0639577e+00 3.0638494e+00 3.0638766e+00 1.0000000e-07 1.0000000e-07\n",
      "  3.0492663e+00 1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07\n",
      "  1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07]\n",
      " [3.0802903e+00 1.0000000e-07 1.0000000e-07 1.0000000e-07 3.0507002e+00\n",
      "  3.0506892e+00 3.0505810e+00 3.0506082e+00 1.0000000e-07 1.0000000e-07\n",
      "  3.0359979e+00 1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07\n",
      "  1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07]\n",
      " [1.0000000e-07 3.0507390e+00 3.0205243e+00 3.0787055e+00 1.0000000e-07\n",
      "  1.0000000e-07 1.0000000e-07 1.0000000e-07 3.0492663e+00 3.0359979e+00\n",
      "  1.0000000e-07 3.0507584e+00 3.0592022e+00 3.0592022e+00 3.0592022e+00\n",
      "  3.0592022e+00 3.0592022e+00 3.0592022e+00 3.0592022e+00 3.0592022e+00]\n",
      " [3.0950506e+00 1.0000000e-07 1.0000000e-07 1.0000000e-07 3.0654607e+00\n",
      "  3.0654497e+00 3.0653415e+00 3.0653687e+00 1.0000000e-07 1.0000000e-07\n",
      "  3.0507584e+00 1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07\n",
      "  1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07]\n",
      " [3.1034946e+00 1.0000000e-07 1.0000000e-07 1.0000000e-07 3.0739045e+00\n",
      "  3.0738935e+00 3.0737853e+00 3.0738125e+00 1.0000000e-07 1.0000000e-07\n",
      "  3.0592022e+00 1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07\n",
      "  1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07]\n",
      " [3.1034946e+00 1.0000000e-07 1.0000000e-07 1.0000000e-07 3.0739045e+00\n",
      "  3.0738935e+00 3.0737853e+00 3.0738125e+00 1.0000000e-07 1.0000000e-07\n",
      "  3.0592022e+00 1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07\n",
      "  1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07]\n",
      " [3.1034946e+00 1.0000000e-07 1.0000000e-07 1.0000000e-07 3.0739045e+00\n",
      "  3.0738935e+00 3.0737853e+00 3.0738125e+00 1.0000000e-07 1.0000000e-07\n",
      "  3.0592022e+00 1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07\n",
      "  1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07]\n",
      " [3.1034946e+00 1.0000000e-07 1.0000000e-07 1.0000000e-07 3.0739045e+00\n",
      "  3.0738935e+00 3.0737853e+00 3.0738125e+00 1.0000000e-07 1.0000000e-07\n",
      "  3.0592022e+00 1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07\n",
      "  1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07]\n",
      " [3.1034946e+00 1.0000000e-07 1.0000000e-07 1.0000000e-07 3.0739045e+00\n",
      "  3.0738935e+00 3.0737853e+00 3.0738125e+00 1.0000000e-07 1.0000000e-07\n",
      "  3.0592022e+00 1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07\n",
      "  1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07]\n",
      " [3.1034946e+00 1.0000000e-07 1.0000000e-07 1.0000000e-07 3.0739045e+00\n",
      "  3.0738935e+00 3.0737853e+00 3.0738125e+00 1.0000000e-07 1.0000000e-07\n",
      "  3.0592022e+00 1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07\n",
      "  1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07]\n",
      " [3.1034946e+00 1.0000000e-07 1.0000000e-07 1.0000000e-07 3.0739045e+00\n",
      "  3.0738935e+00 3.0737853e+00 3.0738125e+00 1.0000000e-07 1.0000000e-07\n",
      "  3.0592022e+00 1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07\n",
      "  1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07]\n",
      " [3.1034946e+00 1.0000000e-07 1.0000000e-07 1.0000000e-07 3.0739045e+00\n",
      "  3.0738935e+00 3.0737853e+00 3.0738125e+00 1.0000000e-07 1.0000000e-07\n",
      "  3.0592022e+00 1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07\n",
      "  1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07 1.0000000e-07]], shape=(20, 20), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "snippet_loss_numerator = pos_sentence_lables_xor*(pos_sentence_labels_repeat*pos_sentence_scores_repeat + pos_sentence_labels_repeat_transpose*pos_sentence_scores_repeat_transpose)\n",
    "print(snippet_loss_numerator[0,:])\n",
    "snippet_loss_denominator = pos_sentence_scores_repeat*pos_sentence_lables_xor + pos_sentence_lables_xor*pos_sentence_scores_repeat_transpose + 0.0000001\n",
    "print(snippet_loss_denominator[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "snippet_loss = snippet_loss_numerator/snippet_loss_denominator\n",
    "snippet_loss = tf.reshape(snippet_loss, (-1,))\n",
    "\n",
    "snippet_loss_mask = snippet_loss>0.01\n",
    "snippet_loss_indices = tf.cast(tf.where(snippet_loss_mask), tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet_loss = tf.gather_nd(snippet_loss, snippet_loss_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2160,), dtype=float32, numpy=\n",
       "array([0.68411934, 0.67430913, 0.69311476, ..., 0.7043274 , 0.7043274 ,\n",
       "       0.7043274 ], dtype=float32)>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snippet_loss = -tf.math.log(snippet_loss)\n",
    "snippet_loss  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.68908364>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snippet_loss = tf.math.reduce_mean(snippet_loss)\n",
    "snippet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.64544165>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snippet_loss = snippet_loss_sum/(snippet_loss_num+ 0.0000001)\n",
    "snippet_loss = tf.math.reduce_mean(snippet_loss)\n",
    "snippet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.65\n",
    "def transformations_for_pairwise(vector):\n",
    "        \n",
    "    vector_t = tf.transpose(vector, perm=[0,2,1])\n",
    "    vector_repeat = tf.repeat(vector, model_cfg[\"max_passages\"], axis=-1)\n",
    "    vector_t_repeat = tf.repeat(vector_t, model_cfg[\"max_passages\"], axis=-2)\n",
    "\n",
    "    vector_xor = tf.cast(tf.math.logical_xor(tf.cast(vector_repeat, tf.bool), \n",
    "                                                          tf.cast(vector_t_repeat, tf.bool)), \n",
    "                                      tf.float32)\n",
    "\n",
    "    vector_repeat = tf.cast(vector_repeat, tf.float32)\n",
    "    vector_t_repeat = tf.cast(vector_t_repeat, tf.float32)\n",
    "\n",
    "    return vector_repeat, vector_t_repeat, vector_xor\n",
    "def joint_loss_pairwise(pos, neg, pos_label, neg_label):\n",
    "        \n",
    "    pos_score = pos[0]\n",
    "    neg_score = neg[0]\n",
    "\n",
    "    p_wise_loss = pairwise_cross_entropy(pos_score, neg_score)\n",
    "\n",
    "    ## PAIRWISE SNIPPET SCORE LOSS\n",
    "\n",
    "    pos_sentence_labels = tf.cast(tf.reshape(pos_label, (-1, model_cfg[\"max_passages\"], 1)), tf.int32)\n",
    "    pos_sentence_scores = tf.cast(tf.math.exp(tf.reshape(pos[1], (-1, model_cfg[\"max_passages\"], 1))), tf.float32)\n",
    "\n",
    "    # labels\n",
    "    pos_sentence_labels_repeat, pos_sentence_labels_repeat_transpose, pos_sentence_lables_xor =  transformations_for_pairwise(pos_sentence_labels)\n",
    "\n",
    "    # scores\n",
    "    pos_sentence_scores_repeat, pos_sentence_scores_repeat_transpose, pos_sentence_scores_xor =  transformations_for_pairwise(pos_sentence_scores)\n",
    "\n",
    "    snippet_loss_numerator = pos_sentence_lables_xor*(pos_sentence_labels_repeat*pos_sentence_scores_repeat + pos_sentence_labels_repeat_transpose*pos_sentence_scores_repeat_transpose)\n",
    "\n",
    "    snippet_loss_denominator = pos_sentence_scores_repeat*pos_sentence_lables_xor + pos_sentence_lables_xor*pos_sentence_scores_repeat_transpose + 0.0000001\n",
    "    \"\"\"\n",
    "    snippet_loss = tf.math.log(snippet_loss_numerator/snippet_loss_denominator)\n",
    "\n",
    "    snippet_loss = tf.reshape(snippet_loss, (-1,model_cfg[\"max_passages\"]*model_cfg[\"max_passages\"]))\n",
    "\n",
    "    snippet_loss_mask = tf.cast(snippet_loss>-5, tf.float32)\n",
    "\n",
    "    snippet_loss = -snippet_loss\n",
    "\n",
    "    snippet_loss_sum = tf.math.reduce_sum(snippet_loss*snippet_loss_mask, axis=-1)\n",
    "    snippet_loss_num = tf.math.reduce_sum(snippet_loss_mask,  axis=-1)\n",
    "\n",
    "\n",
    "    snippet_loss = snippet_loss_sum/(snippet_loss_num+ 0.0000001)\n",
    "\n",
    "    snippet_loss = tf.math.reduce_mean(snippet_loss)\n",
    "    \"\"\"\n",
    "\n",
    "    snippet_loss = snippet_loss_numerator/snippet_loss_denominator\n",
    "    snippet_loss = tf.reshape(snippet_loss, (-1,))\n",
    "\n",
    "    snippet_loss_mask = snippet_loss>0.01\n",
    "    snippet_loss_indices = tf.cast(tf.where(snippet_loss_mask), tf.int32)\n",
    "\n",
    "    snippet_loss = tf.gather_nd(snippet_loss, snippet_loss_indices)\n",
    "\n",
    "    snippet_loss = -tf.math.log(snippet_loss)\n",
    "\n",
    "    snippet_loss = tf.math.reduce_mean(snippet_loss)\n",
    "\n",
    "    return (gamma * p_wise_loss) + ((1-gamma) * snippet_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "@tf.function\n",
    "def train_step(pos_doc, neg_doc):\n",
    "    \n",
    "\n",
    "    y_pos_doc = pos_doc[:3]\n",
    "    label_pos_doc_snppets = pos_doc[3]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        pos = ranking_model(y_pos_doc)\n",
    "        neg = ranking_model(neg_doc)\n",
    "\n",
    "        loss = joint_loss_pairwise(pos, neg, label_pos_doc_snppets, None)\n",
    "\n",
    "\n",
    "    grads = tape.gradient(loss, ranking_model.trainable_weights)\n",
    "    #print(grads)\n",
    "    optimizer.apply_gradients(zip(grads, ranking_model.trainable_weights))\n",
    "    return loss\n",
    "        #input(\"Press Enter to continue...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.6913051, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6843848, shape=(), dtype=float32)\n",
      "tf.Tensor(0.67998147, shape=(), dtype=float32)\n",
      "tf.Tensor(0.68046916, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6847, shape=(), dtype=float32)\n",
      "tf.Tensor(0.68611443, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6825864, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6823709, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6823955, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6734829, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6774239, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6797923, shape=(), dtype=float32)\n",
      "tf.Tensor(0.682059, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6846226, shape=(), dtype=float32)\n",
      "tf.Tensor(0.685148, shape=(), dtype=float32)\n",
      "tf.Tensor(0.67517567, shape=(), dtype=float32)\n",
      "tf.Tensor(0.673415, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6837712, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6749089, shape=(), dtype=float32)\n",
      "tf.Tensor(0.66995656, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6668084, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6843816, shape=(), dtype=float32)\n",
      "tf.Tensor(0.67835605, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6860029, shape=(), dtype=float32)\n",
      "tf.Tensor(0.65471506, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6661822, shape=(), dtype=float32)\n",
      "tf.Tensor(0.674491, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6642586, shape=(), dtype=float32)\n",
      "tf.Tensor(0.66213906, shape=(), dtype=float32)\n",
      "tf.Tensor(0.677287, shape=(), dtype=float32)\n",
      "tf.Tensor(0.66906655, shape=(), dtype=float32)\n",
      "tf.Tensor(0.66907746, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6913123, shape=(), dtype=float32)\n",
      "tf.Tensor(0.68215466, shape=(), dtype=float32)\n",
      "tf.Tensor(0.67834425, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7023291, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6866014, shape=(), dtype=float32)\n",
      "tf.Tensor(0.658662, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6533773, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6862385, shape=(), dtype=float32)\n",
      "tf.Tensor(0.65497386, shape=(), dtype=float32)\n",
      "tf.Tensor(0.67199546, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6589987, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6752307, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6706988, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6728184, shape=(), dtype=float32)\n",
      "tf.Tensor(0.67053914, shape=(), dtype=float32)\n",
      "tf.Tensor(0.66486937, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6420946, shape=(), dtype=float32)\n",
      "tf.Tensor(0.66407216, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6675489, shape=(), dtype=float32)\n",
      "tf.Tensor(0.64984626, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6624851, shape=(), dtype=float32)\n",
      "tf.Tensor(0.64273596, shape=(), dtype=float32)\n",
      "tf.Tensor(0.644474, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6632555, shape=(), dtype=float32)\n",
      "tf.Tensor(0.62751675, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6085932, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6897111, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6292411, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6933669, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6490527, shape=(), dtype=float32)\n",
      "tf.Tensor(0.65584934, shape=(), dtype=float32)\n",
      "tf.Tensor(0.5989196, shape=(), dtype=float32)\n",
      "tf.Tensor(0.62126684, shape=(), dtype=float32)\n",
      "tf.Tensor(0.63285935, shape=(), dtype=float32)\n",
      "tf.Tensor(0.65330684, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6610668, shape=(), dtype=float32)\n",
      "tf.Tensor(0.64305073, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6577325, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6492321, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6717838, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6242217, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6425108, shape=(), dtype=float32)\n",
      "tf.Tensor(0.62851715, shape=(), dtype=float32)\n",
      "tf.Tensor(0.61825085, shape=(), dtype=float32)\n",
      "tf.Tensor(0.670903, shape=(), dtype=float32)\n",
      "tf.Tensor(0.62961686, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6644951, shape=(), dtype=float32)\n",
      "tf.Tensor(0.62229055, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6104612, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6559093, shape=(), dtype=float32)\n",
      "tf.Tensor(0.69312257, shape=(), dtype=float32)\n",
      "tf.Tensor(0.65789276, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6467515, shape=(), dtype=float32)\n",
      "tf.Tensor(0.60278904, shape=(), dtype=float32)\n",
      "tf.Tensor(0.67263657, shape=(), dtype=float32)\n",
      "tf.Tensor(0.5938807, shape=(), dtype=float32)\n",
      "tf.Tensor(0.5794605, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6708442, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6319389, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6254818, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6619806, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6066622, shape=(), dtype=float32)\n",
      "tf.Tensor(0.65846777, shape=(), dtype=float32)\n",
      "tf.Tensor(0.60833794, shape=(), dtype=float32)\n",
      "tf.Tensor(0.60828704, shape=(), dtype=float32)\n",
      "tf.Tensor(0.62553483, shape=(), dtype=float32)\n",
      "tf.Tensor(0.64425784, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6108006, shape=(), dtype=float32)\n",
      "tf.Tensor(0.5985804, shape=(), dtype=float32)\n",
      "tf.Tensor(0.57633185, shape=(), dtype=float32)\n",
      "tf.Tensor(0.54493463, shape=(), dtype=float32)\n",
      "tf.Tensor(0.5900457, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6218359, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6363059, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6481651, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6531439, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6516254, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6295938, shape=(), dtype=float32)\n",
      "tf.Tensor(0.627463, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6331663, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6672635, shape=(), dtype=float32)\n",
      "tf.Tensor(0.661611, shape=(), dtype=float32)\n",
      "tf.Tensor(0.5869369, shape=(), dtype=float32)\n",
      "tf.Tensor(0.58881605, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6066352, shape=(), dtype=float32)\n",
      "tf.Tensor(0.63211864, shape=(), dtype=float32)\n",
      "tf.Tensor(0.62888855, shape=(), dtype=float32)\n",
      "tf.Tensor(0.58538234, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6516869, shape=(), dtype=float32)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5936eca066a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpos_doc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_doc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/BioASQ-9b/py-bioasq/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/BioASQ-9b/py-bioasq/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/BioASQ-9b/py-bioasq/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/BioASQ-9b/py-bioasq/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/BioASQ-9b/py-bioasq/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/BioASQ-9b/py-bioasq/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gen = train_collection.generator()\n",
    "\n",
    "for i in range(800):\n",
    "    pos_doc, neg_doc = next(gen)\n",
    "    \n",
    "    loss = train_step(pos_doc, neg_doc)\n",
    "    \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-bioasq",
   "language": "python",
   "name": "py-bioasq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
