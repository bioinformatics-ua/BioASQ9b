{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmnrm.utils import set_random_seed, load_neural_model, load_model, flat_list\n",
    "from mmnrm.dataset import TestCollectionV2, sentence_splitter_builderV2\n",
    "from mmnrm.evaluation import BioASQ_Evaluator\n",
    "\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "def build_data_generators(tokenizer, use_joint=True, queries_sw=None, docs_sw=None):\n",
    "    \n",
    "    punkt_sent_tokenizer = PunktSentenceTokenizer().span_tokenize\n",
    "    \n",
    "    def sent_tokenize(document):\n",
    "        return [ document[start:end] for start,end in punkt_sent_tokenizer(document) ]\n",
    "    \n",
    "    def maybe_tokenize(documents):\n",
    "        if \"tokens\" not in documents:\n",
    "            if use_joint:\n",
    "                split = sent_tokenize(documents[\"text\"])\n",
    "            else:\n",
    "                split = nltk.sent_tokenize(documents[\"text\"])\n",
    "                \n",
    "            documents[\"tokens\"] = tokenizer.texts_to_sequences(split)\n",
    "            if docs_sw is not None:\n",
    "                for tokenized_sentence in documents[\"tokens\"]:\n",
    "                    tokenized_sentence = [token for token in tokenized_sentence if token not in docs_sw]\n",
    "    \n",
    "    def train_generator(data_generator):\n",
    "        while True:\n",
    "\n",
    "            # get the batch triplet\n",
    "            query, pos_docs, neg_docs = next(data_generator)\n",
    "\n",
    "            # tokenization, this can be cached for efficientcy porpuses NOTE!!\n",
    "            tokenized_query = tokenizer.texts_to_sequences(query)\n",
    "\n",
    "            if queries_sw is not None:\n",
    "                for tokens in tokenized_query:\n",
    "                    tokenized_query = [token for token in tokens if token not in queries_sw] \n",
    "            \n",
    "            saveReturn = True\n",
    "            \n",
    "            for batch_index in range(len(pos_docs)):\n",
    "                \n",
    "                # tokenizer with cache in [batch_index][tokens]\n",
    "                maybe_tokenize(pos_docs[batch_index])\n",
    "                \n",
    "                # assertion\n",
    "                if all([ len(sentence)==0  for sentence in pos_docs[batch_index][\"tokens\"]]):\n",
    "                    saveReturn = False\n",
    "                    break # try a new resampling, NOTE THIS IS A EASY FIX PLS REDO THIS!!!!!!!\n",
    "                          # for obvious reasons\n",
    "                \n",
    "                maybe_tokenize(neg_docs[batch_index])\n",
    "                \n",
    "            if saveReturn: # this is not true, if the batch is rejected\n",
    "                yield tokenized_query, pos_docs, neg_docs\n",
    "\n",
    "    def test_generator(data_generator):\n",
    "        for _id, query, docs in data_generator:\n",
    "            tokenized_queries = []\n",
    "            for i in range(len(_id)):\n",
    "                # tokenization\n",
    "                tokenized_query = tokenizer.texts_to_sequences([query[i]])[0]\n",
    "\n",
    "                if queries_sw is not None:\n",
    "                    tokenized_query = [token for token in tokenized_query if token not in queries_sw] \n",
    "                \n",
    "                tokenized_queries.append(tokenized_query)\n",
    "                    \n",
    "        \n",
    "                for doc in docs[i]:\n",
    "                    maybe_tokenize(doc)\n",
    "                                                 \n",
    "            yield _id, tokenized_queries, docs\n",
    "            \n",
    "    return train_generator, test_generator\n",
    "\n",
    "def model_train_generator_for_model(model):\n",
    "\n",
    "    if \"model\" in model.savable_config:\n",
    "        cfg = model.savable_config[\"model\"]\n",
    "    \n",
    "    train_gen, test_gen = build_data_generators(model.tokenizer)\n",
    "    \n",
    "    pad_tokens = lambda x, max_len, dtype='int32': tf.keras.preprocessing.sequence.pad_sequences(x, \n",
    "                                                                                           maxlen=max_len,\n",
    "                                                                                           dtype=dtype, \n",
    "                                                                                           padding='post', \n",
    "                                                                                           truncating='post', \n",
    "                                                                                           value=0)\n",
    "\n",
    "    pad_sentences = lambda x, max_lim, dtype='int32': x[:max_lim] + [[]]*(max_lim-len(x))\n",
    "    \n",
    "    def maybe_padding(document):\n",
    "        if isinstance(document[\"tokens\"], list):\n",
    "            #overflow prevention\n",
    "            bounded_doc_passage = min(cfg[\"max_passages\"],len(document[\"tokens\"]))\n",
    "            document[\"sentences_mask\"] = [True] * bounded_doc_passage + [False] * (cfg[\"max_passages\"]-bounded_doc_passage)\n",
    "            document[\"tokens\"] = pad_tokens(pad_sentences(document[\"tokens\"], cfg[\"max_passages\"]), cfg[\"max_p_terms\"])\n",
    "            \n",
    "    def train_generator(data_generator):\n",
    " \n",
    "        for query, pos_docs, neg_docs in train_gen(data_generator):\n",
    "            \n",
    "            query = pad_tokens(query, cfg[\"max_q_terms\"])\n",
    "            \n",
    "            pos_docs_array = []\n",
    "            pos_docs_mask_array = []\n",
    "            neg_docs_array = []\n",
    "            neg_docs_mask_array = []\n",
    "            \n",
    "            # pad docs, use cache here\n",
    "            for batch_index in range(len(pos_docs)):\n",
    "                maybe_padding(pos_docs[batch_index])\n",
    "                pos_docs_array.append(pos_docs[batch_index][\"tokens\"])\n",
    "                pos_docs_mask_array.append(pos_docs[batch_index][\"sentences_mask\"])\n",
    "                maybe_padding(neg_docs[batch_index])\n",
    "                neg_docs_array.append(neg_docs[batch_index][\"tokens\"])\n",
    "                neg_docs_mask_array.append(neg_docs[batch_index][\"sentences_mask\"])\n",
    "            \n",
    "            yield [query, np.array(pos_docs_array), np.array(pos_docs_mask_array)], [query, np.array(neg_docs_array), np.array(neg_docs_mask_array)]\n",
    "            \n",
    "    def test_generator(data_generator):\n",
    "        \n",
    "        for ids, query, docs in test_gen(data_generator):\n",
    "            \n",
    "            docs_ids = []\n",
    "            docs_array = []\n",
    "            docs_mask_array = []\n",
    "            query_array = []\n",
    "            query_ids = []\n",
    "            \n",
    "            for i in range(len(ids)):\n",
    "                \n",
    "                for doc in docs[i]:\n",
    "                    # pad docs, use cache here\n",
    "                    maybe_padding(doc)\n",
    "                    docs_array.append(doc[\"tokens\"])\n",
    "                    docs_mask_array.append(doc[\"sentences_mask\"])\n",
    "                    docs_ids.append(doc[\"id\"])\n",
    "                \n",
    "                query_tokens = pad_tokens([query[i]], cfg[\"max_q_terms\"])[0]\n",
    "                query_tokens = [query_tokens] * len(docs[i])\n",
    "                query_array.append(query_tokens)\n",
    "                    \n",
    "                query_ids.append([ids[i]]*len(docs[i]))\n",
    "            \n",
    "            #print(np.array(docs_mask_array))\n",
    "            \n",
    "            yield flat_list(query_ids), [np.array(flat_list(query_array)), np.array(docs_array), np.array(docs_mask_array)], docs_ids, None\n",
    "            \n",
    "    return train_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def rank(model, t_collection):\n",
    "\n",
    "    generator_Y = t_collection.generator()\n",
    "                \n",
    "    q_scores = defaultdict(list)\n",
    "\n",
    "    for query_id, Y, docs_ids, offsets_docs in generator_Y:\n",
    "        s_time = time.time()\n",
    "        \n",
    "        scores = model.predict(Y)\n",
    "        scores = scores[:,0].tolist()\n",
    "        \n",
    "        for i in range(len(scores)):\n",
    "            \n",
    "            #q_scores[query_id].extend(list(zip(docs_ids,scores)))\n",
    "            q_scores[query_id[i]].append({\"id\":docs_ids[i],\n",
    "                                          \"score\":scores[i]})\n",
    "        \n",
    "        print(\"\\rEvaluation {} | time {}\".format(len(q_scores), time.time()-s_time), end=\"\\r\")\n",
    "\n",
    "    # sort the rankings\n",
    "    for query_id in q_scores.keys():\n",
    "        q_scores[query_id].sort(key=lambda x:-x[\"score\"])\n",
    "        q_scores[query_id] = q_scores[query_id]\n",
    "    \n",
    "    return q_scores\n",
    "\n",
    "def rerank_run(baseline_file, top_k):\n",
    "    run = load_document_run(baseline_file, dict_format=True)\n",
    "\n",
    "    tCollection = TestCollectionV2(queries, run)\\\n",
    "                      .batch_size(top_k)\\\n",
    "                      .set_transform_inputs_fn(test_input_generator)\n",
    "    \n",
    "    results = rank(ranking_model, tCollection)\n",
    "    \n",
    "    return create_document_run(queries, results) ## update the run results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n"
     ]
    }
   ],
   "source": [
    "queries = load_queries(\"BioASQ-task9bPhaseA-testset1\", maps=[(\"body\",\"query\")])\n",
    "ranking_model = load_model(\"trained_models/comic-morning-47_val_collection0_map@10\")\n",
    "_, test_input_generator = model_train_generator_for_model(ranking_model)\n",
    "\n",
    "rerank = rerank_run(\"runs/rnd1/bm25-baseline-long.run\", 100)\n",
    "print([len(q[\"documents\"]) for q in rerank], min([len(q[\"documents\"]) for q in rerank]), len(rerank))\n",
    "write_as_bioasq(rerank, \"runs/rnd1/BIT.UA-01-long.json\")\n",
    "write_as_trec(rerank, \"runs/rnd1/BIT.UA-01-long.trec\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RRF Fusion\n",
      "File runs/rnd1/BIT.UA-02.trec writen.\n"
     ]
    }
   ],
   "source": [
    "#trained_models/\n",
    "checkpoints_name = [\"comic-morning-47_val_collection0_map@10\",\n",
    "                   \"feasible-glade-44_val_collection0_map@10\",\n",
    "                   \"cosmic-darkness-43_val_collection0_map@10\",\n",
    "                   \"bright-dawn-45_val_collection0_map@10\",\n",
    "                   \"breezy-leaf-46_val_collection0_map@10\"\n",
    "                  ]\n",
    "\n",
    "for checkpoint_name in checkpoints_name:\n",
    "    trec_name = checkpoint_name+\".trec\"\n",
    "    \n",
    "    if not os.path.exists(os.path.join(\"runs\",\"rnd1\",trec_name)):\n",
    "        ranking_model = load_model(os.path.join(\"trained_models\", checkpoint_name))\n",
    "        _, test_input_generator = model_train_generator_for_model(ranking_model)\n",
    "\n",
    "        rerank = rerank_run(\"runs/rnd1/bm25-baseline.run\", 100)\n",
    "        print([len(q[\"documents\"]) for q in rerank], min([len(q[\"documents\"]) for q in rerank]), len(rerank))\n",
    "        write_as_trec(rerank, os.path.join(\"runs\",\"rnd1\",trec_name))\n",
    "\n",
    "        del ranking_model\n",
    "                  \n",
    "# fusion\n",
    "print(\"RRF Fusion\")\n",
    "fusion_rrf([os.path.join(\"runs\",\"rnd1\",p+\".trec\") for p in checkpoints_name], os.path.join(\"runs\",\"rnd1\",\"BIT.UA-02.trec\"))\n",
    "convert_trec_run_to_bioasq(os.path.join(\"runs\",\"rnd1\",\"BIT.UA-02.trec\"),\n",
    "                           queries,\n",
    "                           os.path.join(\"runs\",\"rnd1\",\"BIT.UA-02.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 3\n",
    "convert_trec_run_to_bioasq(os.path.join(\"runs\",\"rnd1\",\"graceful-donkey-42_val_collection0_map@10.trec\"), \n",
    "                           queries,\n",
    "                           os.path.join(\"runs\",\"rnd1\",\"BIT.UA-03.json\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File runs/rnd1/BIT.UA-04.trec writen.\n"
     ]
    }
   ],
   "source": [
    "# run 4\n",
    "\n",
    "trec_run_names = [\"graceful-donkey-42_val_collection0_map@10.trec\",\n",
    "                  \"graceful-donkey-42_val_collection0_recall@10.trec\"]\n",
    "\n",
    "fusion_rrf([os.path.join(\"runs\",\"rnd1\",t) for t in trec_run_names], \n",
    "            os.path.join(\"runs\",\"rnd1\",\"BIT.UA-04.trec\"))\n",
    "\n",
    "convert_trec_run_to_bioasq(os.path.join(\"runs\",\"rnd1\",\"BIT.UA-04.trec\"), \n",
    "                           queries,\n",
    "                           os.path.join(\"runs\",\"rnd1\",\"BIT.UA-04.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File runs/rnd1/BIT.UA-05.trec writen.\n"
     ]
    }
   ],
   "source": [
    "# run 5\n",
    "trec_run_names = [\"graceful-donkey-42_val_collection0_map@10.trec\",\n",
    "                  \"graceful-donkey-42_val_collection0_recall@10.trec\",\n",
    "                  \"comic-morning-47_val_collection0_map@10.trec\",\n",
    "                  \"feasible-glade-44_val_collection0_map@10.trec\",\n",
    "                  \"bm25-baseline.trec\"\n",
    "                  ]\n",
    "\n",
    "\n",
    "fusion_rrf([os.path.join(\"runs\",\"rnd1\",t) for t in trec_run_names], \n",
    "            os.path.join(\"runs\",\"rnd1\",\"BIT.UA-05.trec\"))\n",
    "\n",
    "convert_trec_run_to_bioasq(os.path.join(\"runs\",\"rnd1\",\"BIT.UA-05.trec\"),\n",
    "                           queries,\n",
    "                           os.path.join(\"runs\",\"rnd1\",\"BIT.UA-05.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_path = \"runs/rnd2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n"
     ]
    }
   ],
   "source": [
    "queries = load_queries(\"BioASQ-task9bPhaseA-testset2\", maps=[(\"body\",\"query\")])\n",
    "\n",
    "ranking_model = load_model(\"trained_models/earthy-glade-11_val_collection0_map@10\")\n",
    "_, test_input_generator = model_train_generator_for_model(ranking_model)\n",
    "\n",
    "rerank = rerank_run(os.path.join(rnd_path, \"bm25-baseline.run\"), 100)\n",
    "print([len(q[\"documents\"]) for q in rerank], min([len(q[\"documents\"]) for q in rerank]), len(rerank))\n",
    "write_as_bioasq(rerank, os.path.join(rnd_path, \"BIT.UA-01.json\"))\n",
    "write_as_trec(rerank, os.path.join(rnd_path, \"BIT.UA-01.trec\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n",
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n"
     ]
    }
   ],
   "source": [
    "queries = load_queries(\"BioASQ-task9bPhaseA-testset2\", maps=[(\"body\",\"query\")])\n",
    "\n",
    "checkpoints_name = [\"earthy-glade-11_val_collection0_map@10\",\n",
    "                    \"easy-pyramid-4_val_collection0_map@10\",\n",
    "                    \"peach-feather-6_val_collection0_map@10\",\n",
    "                    \"glorious-dragon-3_val_collection0_map@10\",\n",
    "                    \"tough-resonance-7_val_collection0_map@10\",\n",
    "                    \"earthy-glade-11_val_collection1_map@10\",\n",
    "                    \"easy-pyramid-4_val_collection1_map@10\",\n",
    "                    \"glorious-dragon-3_val_collection1_map@10\",\n",
    "                    \"tough-resonance-7_val_collection1_map@10\",\n",
    "                    \"peach-feather-6_val_collection1_map@10\",\n",
    "                  ]\n",
    "\n",
    "for checkpoint_name in checkpoints_name:\n",
    "    trec_name = checkpoint_name+\".trec\"\n",
    "    \n",
    "    if not os.path.exists(os.path.join(rnd_path, trec_name)):\n",
    "        ranking_model = load_model(os.path.join(\"trained_models\", checkpoint_name))\n",
    "        _, test_input_generator = model_train_generator_for_model(ranking_model)\n",
    "\n",
    "        rerank = rerank_run(os.path.join(rnd_path, \"bm25-baseline.run\"), 100)\n",
    "        print([len(q[\"documents\"]) for q in rerank], min([len(q[\"documents\"]) for q in rerank]), len(rerank))\n",
    "        write_as_trec(rerank, os.path.join(rnd_path, trec_name))\n",
    "\n",
    "        del ranking_model\n",
    "                  \n",
    "# fusion\n",
    "print(\"RRF Fusion\")\n",
    "fusion_rrf([os.path.join(rnd_path, p+\".trec\") for p in checkpoints_name], os.path.join(rnd_path, \"BIT.UA-02.trec\"))\n",
    "convert_trec_run_to_bioasq(os.path.join(rnd_path, \"BIT.UA-02.trec\"),\n",
    "                           queries,\n",
    "                           os.path.join(rnd_path, \"BIT.UA-02.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = load_queries(\"BioASQ-task9bPhaseA-testset2\", maps=[(\"body\",\"query\")])\n",
    "\n",
    "# run 3\n",
    "#convert_trec_run_to_bioasq(os.path.join(rnd_path,\"wandering-donkey-2_val_collection0_map@10.trec\"), \n",
    "#                           queries,\n",
    "#                           os.path.join(rnd_path,\"BIT.UA-03.json\"))\n",
    "\n",
    "convert_trec_run_to_bioasq(os.path.join(rnd_path,\"graceful-donkey-42_val_collection0_map@10.trec\"), \n",
    "                           queries,\n",
    "                           os.path.join(rnd_path,\"BIT.UA-03-long.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File runs/rnd2/BIT.UA-04.trec writen.\n"
     ]
    }
   ],
   "source": [
    "# run 4\n",
    "\n",
    "queries = load_queries(\"BioASQ-task9bPhaseA-testset2\", maps=[(\"body\",\"query\")])\n",
    "\n",
    "trec_run_names = [\"wandering-donkey-2_val_collection0_map@10.trec\",\n",
    "                  \"wandering-donkey-2_val_collection0_recall@10.trec\",\n",
    "                  \"wandering-donkey-2_val_collection1_recall@10.trec\",\n",
    "                  \"northern-spaceship-17_val_collection0_map@10.trec\",\n",
    "                  \"northern-spaceship-17_val_collection1_map@10.trec\",\n",
    "                  \"northern-spaceship-17_val_collection0_recall@10.trec\",]\n",
    "\n",
    "fusion_rrf([os.path.join(rnd_path,t) for t in trec_run_names], \n",
    "            os.path.join(rnd_path,\"BIT.UA-04.trec\"))\n",
    "\n",
    "convert_trec_run_to_bioasq(os.path.join(rnd_path,\"BIT.UA-04.trec\"), \n",
    "                           queries,\n",
    "                           os.path.join(rnd_path,\"BIT.UA-04.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File runs/rnd2/BIT.UA-05.trec writen.\n"
     ]
    }
   ],
   "source": [
    "#run 5\n",
    "\n",
    "queries = load_queries(\"BioASQ-task9bPhaseA-testset2\", maps=[(\"body\",\"query\")])\n",
    "\n",
    "trec_run_names = [\"earthy-glade-11_val_collection0_map@10.trec\",\n",
    "                  \"easy-pyramid-4_val_collection0_map@10.trec\",\n",
    "                  \"peach-feather-6_val_collection0_map@10.trec\",\n",
    "                  \"wandering-donkey-2_val_collection0_map@10.trec\",\n",
    "                  \"wandering-donkey-2_val_collection0_recall@10.trec\",\n",
    "                  \"wandering-donkey-2_val_collection1_recall@10.trec\",\n",
    "                  \"bm25-baseline.trec\"]\n",
    "\n",
    "fusion_rrf([os.path.join(rnd_path,t) for t in trec_run_names], \n",
    "            os.path.join(rnd_path,\"BIT.UA-05.trec\"))\n",
    "\n",
    "convert_trec_run_to_bioasq(os.path.join(rnd_path,\"BIT.UA-05.trec\"), \n",
    "                           queries,\n",
    "                           os.path.join(rnd_path,\"BIT.UA-05.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_path = \"runs/rnd3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n"
     ]
    }
   ],
   "source": [
    "queries = load_queries(\"BioASQ-task9bPhaseA-testset3\", maps=[(\"body\",\"query\")])\n",
    "\n",
    "ranking_model = load_model(\"trained_models/absurd-snow-7_val_collection0_map@10\")\n",
    "_, test_input_generator = model_train_generator_for_model(ranking_model)\n",
    "\n",
    "rerank = rerank_run(os.path.join(rnd_path, \"bm25-baseline.run\"), 100)\n",
    "print([len(q[\"documents\"]) for q in rerank], min([len(q[\"documents\"]) for q in rerank]), len(rerank))\n",
    "write_as_bioasq(rerank, os.path.join(rnd_path, \"BIT.UA-01.json\"))\n",
    "write_as_trec(rerank, os.path.join(rnd_path, \"BIT.UA-01.trec\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n",
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n",
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n",
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n",
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n",
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n",
      "RRF Fusion\n",
      "File runs/rnd3/BIT.UA-02.trec writen.\n"
     ]
    }
   ],
   "source": [
    "queries = load_queries(\"BioASQ-task9bPhaseA-testset3\", maps=[(\"body\",\"query\")])\n",
    "\n",
    "checkpoints_name = [\"absurd-snow-7_val_collection0_map@10\",\n",
    "                    \"eternal-rain-6_val_collection0_map@10\",\n",
    "                    \"kind-frog-5_val_collection0_map@10\",\n",
    "                    \"grateful-wind-8_val_collection0_recall@10\",\n",
    "                    \"clean-puddle-3_val_collection0_recall@10\",\n",
    "                    \"absurd-snow-7_val_collection0_recall@10\"\n",
    "                  ]\n",
    "\n",
    "for checkpoint_name in checkpoints_name:\n",
    "    trec_name = checkpoint_name+\".trec\"\n",
    "    \n",
    "    if not os.path.exists(os.path.join(rnd_path, trec_name)):\n",
    "        ranking_model = load_model(os.path.join(\"trained_models\", checkpoint_name))\n",
    "        _, test_input_generator = model_train_generator_for_model(ranking_model)\n",
    "\n",
    "        rerank = rerank_run(os.path.join(rnd_path, \"bm25-baseline.run\"), 100)\n",
    "        print([len(q[\"documents\"]) for q in rerank], min([len(q[\"documents\"]) for q in rerank]), len(rerank))\n",
    "        write_as_trec(rerank, os.path.join(rnd_path, trec_name))\n",
    "\n",
    "        del ranking_model\n",
    "                  \n",
    "# fusion\n",
    "print(\"RRF Fusion\")\n",
    "fusion_rrf([os.path.join(rnd_path, p+\".trec\") for p in checkpoints_name], os.path.join(rnd_path, \"BIT.UA-02.trec\"))\n",
    "convert_trec_run_to_bioasq(os.path.join(rnd_path, \"BIT.UA-02.trec\"),\n",
    "                           queries,\n",
    "                           os.path.join(rnd_path, \"BIT.UA-02.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = load_queries(\"BioASQ-task9bPhaseA-testset3\", maps=[(\"body\",\"query\")])\n",
    "\n",
    "# run 3\n",
    "#convert_trec_run_to_bioasq(os.path.join(rnd_path,\"swift-dawn-1_val_collection0_map@10.trec\"), \n",
    "#                           queries,\n",
    "#                           os.path.join(rnd_path,\"BIT.UA-03.json\"))\n",
    "\n",
    "convert_trec_run_to_bioasq(os.path.join(rnd_path,\"graceful-donkey-42_val_collection0_map@10.trec\"), \n",
    "                           queries,\n",
    "                           os.path.join(rnd_path,\"BIT.UA-03-long.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File runs/rnd3/BIT.UA-04.trec writen.\n"
     ]
    }
   ],
   "source": [
    "# run 4\n",
    "\n",
    "queries = load_queries(\"BioASQ-task9bPhaseA-testset3\", maps=[(\"body\",\"query\")])\n",
    "\n",
    "trec_run_names = [\"swift-dawn-1_val_collection0_map@10.trec\",\n",
    "                  \"swift-dawn-1_val_collection0_recall@10.trec\",\n",
    "                  \"swift-dawn-1_val_collection1_recall@10.trec\"]\n",
    "\n",
    "fusion_rrf([os.path.join(rnd_path,t) for t in trec_run_names], \n",
    "            os.path.join(rnd_path,\"BIT.UA-04.trec\"))\n",
    "\n",
    "convert_trec_run_to_bioasq(os.path.join(rnd_path,\"BIT.UA-04.trec\"), \n",
    "                           queries,\n",
    "                           os.path.join(rnd_path,\"BIT.UA-04.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File runs/rnd3/BIT.UA-05.trec writen.\n"
     ]
    }
   ],
   "source": [
    "# run 5\n",
    "\n",
    "queries = load_queries(\"BioASQ-task9bPhaseA-testset3\", maps=[(\"body\",\"query\")])\n",
    "\n",
    "trec_run_names = [\"absurd-snow-7_val_collection0_map@10.trec\",\n",
    "                  \"eternal-rain-6_val_collection0_map@10.trec\",\n",
    "                  \"kind-frog-5_val_collection0_map@10.trec\",\n",
    "                  \"swift-dawn-1_val_collection0_map@10.trec\",\n",
    "                  \"swift-dawn-1_val_collection0_recall@10.trec\",\n",
    "                  \"swift-dawn-1_val_collection1_recall@10.trec\",\n",
    "                  \"bm25-baseline.trec\"]\n",
    "\n",
    "fusion_rrf([os.path.join(rnd_path,t) for t in trec_run_names], \n",
    "            os.path.join(rnd_path,\"BIT.UA-05.trec\"))\n",
    "\n",
    "convert_trec_run_to_bioasq(os.path.join(rnd_path,\"BIT.UA-05.trec\"), \n",
    "                           queries,\n",
    "                           os.path.join(rnd_path,\"BIT.UA-05.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_path = \"runs/rnd4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n",
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n",
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n",
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n",
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n",
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n",
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n",
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n",
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n",
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n",
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n",
      "RRF Fusion\n",
      "File runs/rnd4/BIT.UA-01.trec writen.\n"
     ]
    }
   ],
   "source": [
    "queries = load_queries(\"BioASQ-task9bPhaseA-testset4\", maps=[(\"body\",\"query\")])\n",
    "\n",
    "checkpoints_name = [\"stilted-night-3_val_collection0_map@10\",\n",
    "                    \"trim-energy-7_val_collection0_map@10\",\n",
    "                    \"dashing-snowflake-6_val_collection0_map@10\",\n",
    "                    \"sage-serenity-2_val_collection0_map@10\",\n",
    "                    \"copper-yogurt-5_val_collection0_map@10\",\n",
    "                    \"quiet-thunder-4_val_collection0_map@10\",\n",
    "                    \"stilted-night-3_val_collection1_recall@10\",\n",
    "                    \"trim-energy-7_val_collection1_recall@10\",\n",
    "                    \"dashing-snowflake-6_val_collection1_recall@10\",\n",
    "                    \"sage-serenity-2_val_collection1_recall@10\",\n",
    "                    \"copper-yogurt-5_val_collection1_recall@10\",\n",
    "                    \"quiet-thunder-4_val_collection1_recall@10\"\n",
    "                  ]\n",
    "\n",
    "for checkpoint_name in checkpoints_name:\n",
    "    trec_name = checkpoint_name+\".trec\"\n",
    "    \n",
    "    if not os.path.exists(os.path.join(rnd_path, trec_name)):\n",
    "        ranking_model = load_model(os.path.join(\"trained_models\", checkpoint_name))\n",
    "        _, test_input_generator = model_train_generator_for_model(ranking_model)\n",
    "\n",
    "        rerank = rerank_run(os.path.join(rnd_path, \"bm25-baseline.run\"), 100)\n",
    "        print([len(q[\"documents\"]) for q in rerank], min([len(q[\"documents\"]) for q in rerank]), len(rerank))\n",
    "        write_as_trec(rerank, os.path.join(rnd_path, trec_name))\n",
    "\n",
    "        del ranking_model\n",
    "                  \n",
    "# fusion\n",
    "print(\"RRF Fusion\")\n",
    "fusion_rrf([os.path.join(rnd_path, p+\".trec\") for p in checkpoints_name], os.path.join(rnd_path, \"BIT.UA-01.trec\"))\n",
    "convert_trec_run_to_bioasq(os.path.join(rnd_path, \"BIT.UA-01.trec\"),\n",
    "                           queries,\n",
    "                           os.path.join(rnd_path, \"BIT.UA-01.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File runs/rnd4/BIT.UA-03.trec writen.\n"
     ]
    }
   ],
   "source": [
    "# run 3\n",
    "\n",
    "queries = load_queries(\"BioASQ-task9bPhaseA-testset4\", maps=[(\"body\",\"query\")])\n",
    "\n",
    "trec_run_names = [\"brisk-butterfly-1_val_collection0_map@10.trec\",\n",
    "                  \"brisk-butterfly-1_val_collection0_recall@10.trec\",\n",
    "                  \"dandy-elevator-14_val_collection0_map@10.trec\",\n",
    "                  \"dandy-elevator-14_val_collection0_recall@10.trec\",]\n",
    "\n",
    "fusion_rrf([os.path.join(rnd_path,t) for t in trec_run_names], \n",
    "            os.path.join(rnd_path,\"BIT.UA-03.trec\"))\n",
    "\n",
    "convert_trec_run_to_bioasq(os.path.join(rnd_path,\"BIT.UA-03.trec\"), \n",
    "                           queries,\n",
    "                           os.path.join(rnd_path,\"BIT.UA-03.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_path = \"runs/rnd5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n",
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n",
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n",
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n",
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n",
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n",
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n",
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n",
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n",
      "DEBUG created tokenizer bioasq_9b_RegexTokenizer\n",
      "False False\n",
      "[LOAD FROM CACHE] Load embedding matrix from /backup/BioASQ-9b/embeddings/WORD2VEC_embedding_bioasq_9b_gensim_iter_15_freq0_200_Regex_word2vec_bioasq_9b_RegexTokenizer\n",
      "Using einsum for mask bq,bps->bpqs and with embedding dim bqe,bpse->bpqs\n",
      "[EMBEDDING MATRIX SHAPE] (5322623, 200)\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 100 100\n",
      "RRF Fusion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File runs/rnd5/BIT.UA-01.trec writen.\n"
     ]
    }
   ],
   "source": [
    "queries = load_queries(\"BioASQ-task9bPhaseA-testset5\", maps=[(\"body\",\"query\")])\n",
    "\n",
    "checkpoints_name = [\"misty-plant-8_val_collection1_map@10\",\n",
    "                    \"gentle-monkey-6_val_collection1_map@10\",\n",
    "                    \"hopeful-oath-3_val_collection1_map@10\",\n",
    "                    \"stellar-silence-5_val_collection1_map@10\",\n",
    "                    \"distinctive-lake-7_val_collection1_map@10\",\n",
    "                    \"misty-plant-8_val_collection1_recall@10\",\n",
    "                    \"gentle-monkey-6_val_collection1_recall@10\",\n",
    "                    \"hopeful-oath-3_val_collection1_recall@10\",\n",
    "                    \"stellar-silence-5_val_collection1_recall@10\",\n",
    "                    \"distinctive-lake-7_val_collection1_recall@10\",\n",
    "                  ]\n",
    "\n",
    "for checkpoint_name in checkpoints_name:\n",
    "    trec_name = checkpoint_name+\".trec\"\n",
    "    \n",
    "    if not os.path.exists(os.path.join(rnd_path, trec_name)):\n",
    "        ranking_model = load_model(os.path.join(\"trained_models\", checkpoint_name))\n",
    "        _, test_input_generator = model_train_generator_for_model(ranking_model)\n",
    "\n",
    "        rerank = rerank_run(os.path.join(rnd_path, \"bm25-baseline.run\"), 100)\n",
    "        print([len(q[\"documents\"]) for q in rerank], min([len(q[\"documents\"]) for q in rerank]), len(rerank))\n",
    "        write_as_trec(rerank, os.path.join(rnd_path, trec_name))\n",
    "\n",
    "        del ranking_model\n",
    "                  \n",
    "# fusion\n",
    "print(\"RRF Fusion\")\n",
    "fusion_rrf([os.path.join(rnd_path, p+\".trec\") for p in checkpoints_name], os.path.join(rnd_path, \"BIT.UA-01.trec\"))\n",
    "convert_trec_run_to_bioasq(os.path.join(rnd_path, \"BIT.UA-01.trec\"),\n",
    "                           queries,\n",
    "                           os.path.join(rnd_path, \"BIT.UA-01.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File runs/rnd5/BIT.UA-03.trec writen.\n"
     ]
    }
   ],
   "source": [
    "# run 3\n",
    "\n",
    "queries = load_queries(\"BioASQ-task9bPhaseA-testset5\", maps=[(\"body\",\"query\")])\n",
    "\n",
    "trec_run_names = [\"iconic-wave-1_val_collection0_map@10.trec\",\n",
    "                  \"iconic-wave-1_val_collection0_recall@10.trec\"]\n",
    "\n",
    "fusion_rrf([os.path.join(rnd_path,t) for t in trec_run_names], \n",
    "            os.path.join(rnd_path,\"BIT.UA-03.trec\"))\n",
    "\n",
    "convert_trec_run_to_bioasq(os.path.join(rnd_path,\"BIT.UA-03.trec\"), \n",
    "                           queries,\n",
    "                           os.path.join(rnd_path,\"BIT.UA-03.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-bioasq",
   "language": "python",
   "name": "py-bioasq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
